---
layout: post
title: Metis Chicago - Weeks 2 and 3, Project Luther
---

## Project 2:
<br>
Predicting average open table prices in Chicago restaurants from user ratings..
<br>
### Duration:
<br>
2 weeks
<br>
### Objective:
<br>
Webscraping and linear regression. (Or, as I like to call it, pulling my hair out and linear regression).
<br>
## My project:
<br>
For this project, as mentioned above, I chose to take a look at Open Table data.
<br>
<img src= "https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&ved=0ahUKEwj9vr7tnM_TAhVm2IMKHSfPAcAQjRwIBw&url=https%3A%2F%2Fwww.opentable.com%2Fabout%2F&psig=AFQjCNFo4K-dEUux2C6ooyRJyIVKCWI3OQ&ust=1493746330517663" alt='config.yml'>
<br>
Open Table is a website/app that allows users to make reservations at restaurants across the city. Once they've been to a restaurant they can provide their feedback on what they thought about different aspects of the restaurant in five ways: overall rating, food rating, noise rating, service rating, and ambience rating.
<br>
I decided to use each of these factors to try to predict what the average menu price at restaurants around Chicago would be. I hypothesized that specific user ratings (overall and food) were more likely to be better predictors of price than other ratings (noise, ambience, service).
<br>
## Webscraping:
<br>
I don't want to talk about it.
<br>
Ok, but really, this was definitely a steep learning curve for me. So in class we learned about a couple different tools that would help us in our webscraping endeavors: **BeautifulSoup and Selenium**.
<br>
So even though scraping was a really frustrating process for me, it was actually pretty cool to feel like I was hacking! None of my friends or family cared but I showed them anyway.
<br>
My biggest obstacles that I ran into was using Selenium.
<br>
1. My driver opened each link in a new tab and so I had to figure out a way to switch to the most recently opened tab to scrape the data. Otherwise, it would just keep trying to scrape the data from the first page and I would get an error.
2. I had trouble figuring out how to get my code to click "next" at the bottom of the page to then move on to the next page and scrape data.
3. When I figured THAT out I ran into an issue where there was a restaurant called ***Next***. So once it got to the Ns it would just get stuck in an infinite loop of clicking the restaurant called ***Next***.
<br>
But I finished, with a day to spare. Phew.
<br>
## Analyses:
<br>
Per the assignment we had to conduct a linear regression on our data even if it didn't best fit the data.
<br>
For me, my target variable, price, was not normally distributed; an assumption of a running a linear regression analysis. Because of this, I transformed it using a log transformation.
<br>
###Before:
![pre-log](/Users/Dangaia/Documents/Metis/Dangaia.github.io/images/download.png)
###After:
![post-log](/Users/Dangaia/Documents/Metis/Dangaia.github.io/images/download (1).png)
<br>
## Results:
<br>
So it ended up that the model that I expected was not the model that I ended up with.
![/Users/Dangaia/Documents/Metis/Dangaia.github.io/images/Model]()
<br>
When I started the analyses my R<sup>2</sup> value was 0.96! WHAT THE!?! Yeah, turns out that was overfitting. I conducted some regularization and cross-validation and it ended up that my final R<sup>2</sup> was .20.
<br>
This is potentially useful for new restaurants looking to price their menu. That is, if a restaurant wants to be in a specific price category, it gives them information on what user factors they should most focus on.
<br>
## Next Steps:
* Check for features that are correlated to avoid multicollinearity.
<br>
    * Some of my ratings variables were correlated with the overall rating variable. I would go back and check that and potentially run some different analyses.
<br>
* Collect more data.
<br>
    * There are over 7,500 restaurants in Chicago, however, for my project I was only able to scrape complete data for 341.
      1. Nearly 5,000 restaurants don't have any data available on Open Table.
      2. In cleaning my data I decided to remove rows with any NaN values.
<br>
* Examine other variables that my explain the variance in y. Some of the following may be good variables to take a look at. Some of this data is available on Open Table but not all of it.
<br>
    * Location
    * Cuisine type
    * Dress code
    * Time since opening
    * Number of awards received
* Use price as a predictor.
    * Price would be a really interesting variable to use to predict rating. Maybe I'll take a look at that in the future.
<br>
## Final thoughts:
I never thought this project would end. But because of it I have a new found respect for data scientists who have ever scraped even one piece of data. Luckily, I feel ***MUCH*** more confident in my scraping abilities. And moving forward I'm super glad that I struggled through it. It makes me that much more confident that I'll know what I'm doing moving forward.
<br>
# THE END.
